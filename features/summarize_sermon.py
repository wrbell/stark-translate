#!/usr/bin/env python3
"""
summarize_sermon.py — Post-Sermon 5-Sentence Summary Generator

Reads a full sermon transcript (from dry_run_ab.py CSV or diarized JSONL)
and generates a structured summary using a local LLM via MLX.

Summary format (with diarization):
  - Sentence 1: Overall gist covering both speakers' themes
  - Sentences 2-3: Summary of Speaker 1's key points
  - Sentences 4-5: Summary of Speaker 2's key points

Summary format (without diarization):
  - 3-sentence overall summary

Produces both English and Spanish summaries. Spanish is generated by
asking the LLM to translate, or by using the TranslateGemma pipeline.

Output: JSON file in metrics/sermon_summaries/

Requirements:
  - mlx-lm (pip install mlx-lm)
  - A small MLX chat model (default: mlx-community/gemma-2-2b-it-4bit)

Usage:
    python summarize_sermon.py metrics/ab_metrics_20260208_183356.csv
    python summarize_sermon.py metrics/diarization/sermon.jsonl
    python summarize_sermon.py metrics/ab_metrics_*.csv --model mlx-community/Llama-3.2-3B-Instruct-4bit
    python summarize_sermon.py transcript.jsonl --translate-with-gemma
"""

import argparse
import csv
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path


# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------

OUTPUT_DIR = "metrics/sermon_summaries"

# Default summarization model — small, fast, fits easily in 18GB with other models
DEFAULT_MODEL = "mlx-community/gemma-2-2b-it-4bit"

# Max tokens for transcript fed to the LLM (leave room for generation)
MAX_TRANSCRIPT_TOKENS = 3000  # ~12K characters, safe for 8K context models
MAX_TRANSCRIPT_CHARS = 12000

# TranslateGemma model for high-quality translation (optional)
TRANSLATE_MODEL_ID = "mlx-community/translategemma-4b-it-4bit"


# ---------------------------------------------------------------------------
# Transcript Loading
# ---------------------------------------------------------------------------

def load_csv_transcript(csv_path):
    """Load transcript from a dry_run_ab.py CSV file.

    CSV columns: chunk_id, timestamp, english, spanish_a, spanish_b, ...

    Returns:
        list of dicts with 'timestamp', 'text', 'speaker' (None for CSV).
    """
    entries = []
    with open(csv_path, "r", newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            text = row.get("english", "").strip()
            if not text:
                continue
            entries.append({
                "timestamp": row.get("timestamp", ""),
                "text": text,
                "speaker": None,  # CSV has no speaker diarization
            })

    if not entries:
        print(f"WARNING: No transcript entries found in {csv_path}", file=sys.stderr)
    else:
        print(f"  Loaded {len(entries)} segments from CSV")

    return entries


def load_jsonl_transcript(jsonl_path):
    """Load transcript from a diarized JSONL file (output of diarize.py).

    Each line is a JSON object with: speaker, start, end, text, confidence.
    First line may be a metadata header (has '_metadata' key).

    Returns:
        list of dicts with 'timestamp', 'text', 'speaker'.
    """
    entries = []
    with open(jsonl_path, "r") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            record = json.loads(line)

            # Skip metadata header
            if "_metadata" in record:
                continue

            text = record.get("text", "")
            if not text or text.startswith("[transcription error"):
                continue

            # Format timestamp from seconds
            start = record.get("start", 0)
            ts = _format_timestamp(start)

            entries.append({
                "timestamp": ts,
                "text": text,
                "speaker": record.get("speaker"),
            })

    if not entries:
        print(f"WARNING: No transcript entries found in {jsonl_path}", file=sys.stderr)
    else:
        speakers = set(e["speaker"] for e in entries if e["speaker"])
        print(f"  Loaded {len(entries)} segments from JSONL")
        if speakers:
            print(f"  Speakers: {', '.join(sorted(speakers))}")

    return entries


def _format_timestamp(seconds):
    """Format seconds as HH:MM:SS."""
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    return f"{h:02d}:{m:02d}:{s:02d}"


def load_transcript(input_path):
    """Auto-detect file type and load transcript."""
    p = Path(input_path)
    if p.suffix.lower() == ".csv":
        return load_csv_transcript(input_path)
    elif p.suffix.lower() in (".jsonl", ".json"):
        return load_jsonl_transcript(input_path)
    else:
        print(f"ERROR: Unsupported file type: {p.suffix}", file=sys.stderr)
        print(f"  Supported: .csv (dry_run_ab.py output), .jsonl (diarize.py output)",
              file=sys.stderr)
        sys.exit(1)


# ---------------------------------------------------------------------------
# Transcript Preparation
# ---------------------------------------------------------------------------

def has_diarization(entries):
    """Check if transcript entries have speaker labels."""
    speakers = set(e["speaker"] for e in entries if e.get("speaker"))
    return len(speakers) >= 2


def build_transcript_text(entries, max_chars=MAX_TRANSCRIPT_CHARS):
    """Build a plain text transcript, optionally with speaker labels.

    Truncates from the middle if the transcript exceeds max_chars,
    keeping the beginning and end for better summary quality.
    """
    lines = []
    for e in entries:
        if e.get("speaker"):
            lines.append(f"{e['speaker']}: {e['text']}")
        else:
            lines.append(e["text"])

    full_text = "\n".join(lines)

    if len(full_text) <= max_chars:
        return full_text

    # Truncate from the middle — keep beginning and end
    half = max_chars // 2
    truncated = full_text[:half] + "\n\n[... middle portion omitted for brevity ...]\n\n" + full_text[-half:]
    print(f"  Transcript truncated: {len(full_text)} -> {len(truncated)} chars")
    return truncated


def get_speaker_texts(entries):
    """Split transcript by speaker for per-speaker summaries.

    Returns dict mapping speaker_label -> concatenated text.
    """
    speaker_texts = {}
    for e in entries:
        speaker = e.get("speaker", "Unknown")
        if speaker not in speaker_texts:
            speaker_texts[speaker] = []
        speaker_texts[speaker].append(e["text"])

    return {spk: " ".join(texts) for spk, texts in speaker_texts.items()}


# ---------------------------------------------------------------------------
# LLM Summarization (MLX)
# ---------------------------------------------------------------------------

def load_summarization_model(model_id):
    """Load a small chat LLM via mlx-lm for summarization."""
    from mlx_lm import load
    import mlx.core as mx
    mx.set_cache_limit(100 * 1024 * 1024)

    print(f"  Loading {model_id}...")
    t0 = time.time()
    model, tokenizer = load(model_id)
    print(f"  Model ready ({time.time()-t0:.1f}s)")
    return model, tokenizer


def generate_text(model, tokenizer, prompt, max_tokens=512):
    """Generate text using mlx-lm."""
    from mlx_lm import generate

    messages = [{"role": "user", "content": prompt}]
    chat_prompt = tokenizer.apply_chat_template(
        messages, add_generation_prompt=True
    )

    result = generate(
        model, tokenizer,
        prompt=chat_prompt,
        max_tokens=max_tokens,
        verbose=False,
    )

    # Clean up: remove any trailing special tokens
    clean = result.split("<end_of_turn>")[0].strip()
    clean = clean.split("<|eot_id|>")[0].strip()
    clean = clean.split("<|end|>")[0].strip()
    return clean


def summarize_with_diarization(model, tokenizer, transcript_text, speaker_texts):
    """Generate a 5-sentence structured summary with speaker-specific content.

    Returns dict with 'english' and 'spanish' summary strings.
    """
    # Sort speakers by total text length (primary speaker = most text)
    sorted_speakers = sorted(speaker_texts.keys(),
                            key=lambda s: len(speaker_texts[s]),
                            reverse=True)
    spk1 = sorted_speakers[0] if len(sorted_speakers) > 0 else "Speaker 1"
    spk2 = sorted_speakers[1] if len(sorted_speakers) > 1 else "Speaker 2"

    prompt = f"""You are summarizing a church sermon transcript. The sermon has two speakers: {spk1} and {spk2}.

Write exactly 5 sentences:
- Sentence 1: Overall gist covering both speakers' main themes
- Sentences 2-3: Summary of {spk1}'s key points
- Sentences 4-5: Summary of {spk2}'s key points

Be concise and capture the theological content faithfully. Use complete sentences.

Transcript:
{transcript_text}

Write your 5-sentence summary:"""

    print(f"  Generating English summary...")
    t0 = time.time()
    en_summary = generate_text(model, tokenizer, prompt, max_tokens=400)
    print(f"  English summary ready ({time.time()-t0:.1f}s)")

    # Generate Spanish summary
    es_prompt = f"""Translate the following church sermon summary into natural, fluent Spanish. Preserve all theological terms accurately. Use Protestant Spanish conventions (e.g., "pacto" not "alianza" for covenant).

English summary:
{en_summary}

Spanish translation:"""

    print(f"  Generating Spanish summary...")
    t0 = time.time()
    es_summary = generate_text(model, tokenizer, es_prompt, max_tokens=500)
    print(f"  Spanish summary ready ({time.time()-t0:.1f}s)")

    return {
        "english": en_summary,
        "spanish": es_summary,
        "speakers": {
            "primary": spk1,
            "secondary": spk2,
        },
        "format": "5-sentence (diarized)",
    }


def summarize_without_diarization(model, tokenizer, transcript_text):
    """Generate a 3-sentence overall summary (no speaker information).

    Returns dict with 'english' and 'spanish' summary strings.
    """
    prompt = f"""You are summarizing a church sermon transcript.

Write exactly 3 concise sentences capturing the main themes, key scripture references, and central message of the sermon. Use complete sentences.

Transcript:
{transcript_text}

Write your 3-sentence summary:"""

    print(f"  Generating English summary...")
    t0 = time.time()
    en_summary = generate_text(model, tokenizer, prompt, max_tokens=300)
    print(f"  English summary ready ({time.time()-t0:.1f}s)")

    # Generate Spanish summary
    es_prompt = f"""Translate the following church sermon summary into natural, fluent Spanish. Preserve all theological terms accurately. Use Protestant Spanish conventions (e.g., "pacto" not "alianza" for covenant).

English summary:
{en_summary}

Spanish translation:"""

    print(f"  Generating Spanish summary...")
    t0 = time.time()
    es_summary = generate_text(model, tokenizer, es_prompt, max_tokens=400)
    print(f"  Spanish summary ready ({time.time()-t0:.1f}s)")

    return {
        "english": en_summary,
        "spanish": es_summary,
        "speakers": None,
        "format": "3-sentence (undiarized)",
    }


def translate_with_translategemma(en_summary):
    """Use TranslateGemma 4B for higher-quality Spanish translation.

    This is optional — only used if --translate-with-gemma is specified.
    Returns Spanish translation string.
    """
    from mlx_lm import load, generate
    import mlx.core as mx
    mx.set_cache_limit(100 * 1024 * 1024)

    print(f"  Loading TranslateGemma for Spanish translation...")
    t0 = time.time()
    model, tokenizer = load(TRANSLATE_MODEL_ID)

    # Fix EOS token (same as dry_run_ab.py)
    eot_id = tokenizer.convert_tokens_to_ids("<end_of_turn>")
    tokenizer._eos_token_ids = {tokenizer.eos_token_id, eot_id}
    print(f"  TranslateGemma ready ({time.time()-t0:.1f}s)")

    messages = [{"role": "user", "content": [
        {"type": "text",
         "source_lang_code": "en",
         "target_lang_code": "es",
         "text": en_summary}
    ]}]

    prompt = tokenizer.apply_chat_template(
        messages, add_generation_prompt=True
    )

    t0 = time.time()
    result = generate(
        model, tokenizer,
        prompt=prompt,
        max_tokens=600,
        verbose=False,
    )
    clean = result.split("<end_of_turn>")[0].strip()
    print(f"  TranslateGemma translation ready ({(time.time()-t0)*1000:.0f}ms)")
    return clean


# ---------------------------------------------------------------------------
# Output
# ---------------------------------------------------------------------------

def write_summary(summary_data, output_path):
    """Write summary to JSON file."""
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w") as f:
        json.dump(summary_data, f, indent=2, ensure_ascii=False)
    print(f"\n  Output: {output_path}")


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        description="Generate post-sermon 5-sentence summary using local LLM (MLX)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python summarize_sermon.py metrics/ab_metrics_20260208_183356.csv
    python summarize_sermon.py metrics/diarization/sermon.jsonl
    python summarize_sermon.py transcript.csv --model mlx-community/Llama-3.2-3B-Instruct-4bit
    python summarize_sermon.py transcript.jsonl --translate-with-gemma
        """,
    )
    parser.add_argument("input", nargs="+",
                        help="CSV or JSONL transcript file(s)")
    parser.add_argument("--model", type=str, default=DEFAULT_MODEL,
                        help=f"MLX model for summarization (default: {DEFAULT_MODEL})")
    parser.add_argument("--translate-with-gemma", action="store_true",
                        help="Use TranslateGemma 4B for Spanish translation "
                             "instead of the summarization model")
    parser.add_argument("-o", "--output", type=str, default=None,
                        help="Output JSON path (default: metrics/sermon_summaries/<input_name>.json)")
    args = parser.parse_args()

    print(f"{'='*60}")
    print(f"  Post-Sermon Summary Generator")
    print(f"  Model: {args.model}")
    print(f"  Input(s): {len(args.input)} file(s)")
    print(f"{'='*60}\n")

    # Load and merge all input transcripts
    all_entries = []
    for input_path in args.input:
        print(f"Loading: {input_path}")
        entries = load_transcript(input_path)
        all_entries.extend(entries)

    if not all_entries:
        print("ERROR: No transcript content found.", file=sys.stderr)
        sys.exit(1)

    total_words = sum(len(e["text"].split()) for e in all_entries)
    print(f"\n  Total: {len(all_entries)} segments, ~{total_words} words")

    # Check for diarization
    diarized = has_diarization(all_entries)
    if diarized:
        print(f"  Mode: 5-sentence diarized summary")
    else:
        print(f"  Mode: 3-sentence overall summary (no speaker labels)")

    # Build transcript text
    transcript_text = build_transcript_text(all_entries)

    # Load summarization model
    model, tokenizer = load_summarization_model(args.model)

    # Generate summary
    print()
    if diarized:
        speaker_texts = get_speaker_texts(all_entries)
        summary = summarize_with_diarization(model, tokenizer, transcript_text, speaker_texts)
    else:
        summary = summarize_without_diarization(model, tokenizer, transcript_text)

    # Optionally re-translate with TranslateGemma for higher quality
    if args.translate_with_gemma:
        print(f"\n  Re-translating with TranslateGemma...")
        # Free the summarization model first to save memory
        del model, tokenizer
        import gc
        gc.collect()

        es_translation = translate_with_translategemma(summary["english"])
        summary["spanish"] = es_translation
        summary["translation_method"] = "TranslateGemma-4B"
    else:
        summary["translation_method"] = f"{args.model} (direct)"

    # Add metadata
    summary["metadata"] = {
        "input_files": args.input,
        "model": args.model,
        "timestamp": datetime.now().isoformat(),
        "total_segments": len(all_entries),
        "total_words": total_words,
        "diarized": diarized,
    }

    # Determine output path
    output_path = args.output
    if output_path is None:
        # Use first input file's name
        input_name = Path(args.input[0]).stem
        output_path = os.path.join(OUTPUT_DIR, f"{input_name}_summary.json")

    # Write output
    write_summary(summary, output_path)

    # Print results
    print(f"\n{'='*60}")
    print(f"  SERMON SUMMARY")
    print(f"{'='*60}")
    print(f"\n  English ({summary['format']}):")
    print(f"  {'-'*40}")
    for line in summary["english"].split("\n"):
        if line.strip():
            print(f"    {line.strip()}")
    print(f"\n  Spanish:")
    print(f"  {'-'*40}")
    for line in summary["spanish"].split("\n"):
        if line.strip():
            print(f"    {line.strip()}")
    print(f"\n{'='*60}")


if __name__ == "__main__":
    main()
